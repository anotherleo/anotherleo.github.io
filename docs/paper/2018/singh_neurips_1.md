# Fast and Effective Robustness Certification

<center>Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, Martin Vechev</center>

We present a new method and system, called DeepZ, for certifying neural network robustness based on abstract interpretation. Compared to state-of-the-art automated verifiers for neural networks, DeepZ: 

1. handles ReLU, Tanh and Sigmoid activation functions, 
2. supports feedforward, convolutional, and residual architectures, 
3. is significantly more scalable and precise, and
4. is sound with respect to floating point arithmetic. 

These benefits are due to carefully designed approximations tailored to the setting of neural networks. As an example, DeepZ achieves a verification accuracy of 97% on a large network with 88, 500 hidden units under $L_∞$ attack with $\epsilon = 0.1$ with an average runtime of 133 seconds.

## Introduction

Neural networks have become an integral part of many critical applications such as vehicle control, pattern recognition, and medical diagnosis. However, it has been shown recently that neural networks are susceptible to adversarial attacks, where the network can be easily tricked into making wrong classification by only slightly modifying its inputs [26, 12, 17, 3, 2, 9, 23, 22]. As a result, there is considerable interest in formally ensuring robustness of neural networks against such attacks.

Robustness verifiers can be complete or incomplete. 

- Complete verifiers do not have false positives but have limited scalability as they are based on computationally expensive methods such as **SMT solving** [14, 8], **mixed integer linear programming** [27] or **input refinement** [28]. 
- On the other hand, incomplete verifiers can produce false positives but they scale better than complete verifiers. Incomplete verifiers employ a variety of methods including duality [24, 7], abstract interpretation [10], and linear approximations [29, 15].

?> 为什么是incomplete对应false positives？那按照这个定义，测试一定是complete了？

Orthogonal to robustness certification, adversarial training focuses on making neural networks robust by training against an adversarial model of attack. 

- Gu and Rigazio [13] add concrete noise to the training set and remove it statistically for defending against adversarial examples. 
- Goodfellow et al. [12] generate adversarial examples misclassified by neural networks and then design a defense against this attack by explicit training against perturbations generated by the attack. 
- Madry et al. [19] show that training against an optimal attack also guards against non-optimal attacks. 
- While this technique was effective in experiments, Carlini et al. [4] demonstrated an attack for the safety-critical problem of ground-truthing, where this defense occasionally exacerbated the problem. 
- The work of [30] and [21] proposes scalable defenses against the standard $L_∞$-based adversarial attacks.

In this paper we build on the work of Gehr et al. [10], which introduces the idea of using the classic framework of abstract interpretation [5] to soundly approximate the behavior of the network. Similar to [10], we also use the Zonotope abstraction for sound approximations. The Zonotope domain provides a closed affine form for each variable and enables a precise and cheap way to capture the effect of affine transformations inside neural networks, without requiring backpropagation as in [15, 29]. The key limitation of [10] however, is that it only provides a fairly generic abstract transformer for the ReLU activation function, which is slow and imprecise. Further, the work does not discuss approximations of other important functions (e.g., Sigmoid, Tanh). Indeed, defining sound, scalable and precise abstract transformers is the most difficult aspect of abstract-interpretation-based analyses. While generic transformers tend to be easier to reason about and ensure soundness of, they often lack the scalability and precision of transformers that exploit the underlying properties of the abstract domain (e.g., **Zonotope**) and the function being approximated (e.g., ReLU).

**Our contributions.** In this work we address these limitations and make the following contributions:

- We introduce new, point-wise Zonotope abstract transformers specifically designed for the ReLU, Sigmoid, and Tanh activations often used by neural networks. Our transformers minimize the area of the projection of the zonotope to the 2-D input-output plane. Further, our transformers are sound w.r.t. floating point arithmetic.
- We implemented both, sequential and parallel versions of our transformers in an end-to-end automated verification system called DeepZ.
- We evaluated DeepZ on the task of verifying local robustness against $L_∞$-norm based attacks on large MNIST and CIFAR10 feed forward, convolutional, and residual networks. In our evaluation we considered both, undefended as well as defended versions of the same network (defended against $L_∞$ attacks using state-of-the-art defenses).
- Our experimental results demonstrate that DeepZ is more precise and faster than prior work. DeepZ precisely verifies large networks with > 88, 000 hidden units under $L_∞$-norm based perturbations within few minutes, while being sound w.r.t to floating point arithmetic.

To our best knowledge, DeepZ is currently the most scalable system for certifying local robustness of neural networks while guaranteed soundness w.r.t to floating point operations (used by all neural networks). All of our code, datasets and results are publicly available at http://safeai.ethz.ch/.

## Abstract Interpretation for Verifying Robustness of Neural Networks

Abstract Interpretation [5] is a classic method for **sound** and precise **over-approximation** of potentially **unbounded or infinite** set of program behaviors. The key idea behind this framework consists of defining so called abstract transformers for statements used by the program (e.g., affine arithmetic, ReLU functions, etc). These transformers approximate (i.e., they are sound) the behavior of the statement by defining its effect on an abstract domain. An abstract domain is simply a set of abstract elements (approximations) typically ordered in a lattice of some kind.

A key challenge when **defining abstract transformers** is striking a balance between **scalability** (how fast the transformer computes the approximation) and **precision** (how much precision it loses). Once transformers are defined, the analysis with abstract interpretation proceeds by executing them on the particular program (e.g., a neural network) and computing a final approximation (a fixed point). The relevant property can then be checked on this final approximation: if the property can be proved, then it holds for any concrete input to the program, otherwise, it may either hold but the abstraction was too coarse and unable to prove it (i.e., a false positive) or it may indeed not hold.

Verifying robustness properties of neural networks exactly is computationally expensive as it usually requires evaluating the network exhaustively on a prohibitively large set of inputs. Abstract interpretation can be leveraged for this problem by designing abstract transformers specifically for the computations used in the network, e.g., affine arithmetic and activation functions. The network can then be analyzed using these abstract transformers. For example, we can abstract a concrete input $x$ and relevant perturbations to $x$ (resulting in many different inputs) into one abstract element $α_R$ and then analyze the network starting from $α_R$, producing an abstract output $α^o_R$. We can then verify the robustness property of interest over $α^o_R$: if successful, it means we verified it over all concrete outputs corresponding to all perturbations of the concrete input.

## Conclusion

We introduced fast and precise Zonotope abstract transformers for key non-linear activations used in modern neural networks. We used these transformers to build DeepZ, an automated verifier for proving the robustness of neural networks against adversarial attacks. We evaluated the effectiveness of DeepZ on verifying robustness of large feedforward, convolutional, and residual networks against challenging $L_∞$-norm attacks. Our results show that DeepZ is more precise and faster than prior work, while also ensuring soundness with respect to floating point operations.