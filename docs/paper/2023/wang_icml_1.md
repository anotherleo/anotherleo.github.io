# Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments

<center>Yixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, Qi Zhu</center>

Reinforcement Learning (RL) has long grappled with the issue of ensuring agent safety in unpredictable and stochastic environments, particularly under hard constraints that require the system state not to reach unsafe regions. Conventional safe RL methods such as those based on the Constrained Markov Decision Process (CMDP) paradigm formulate safety violations in a cost function and try to constrain the expectation of cumulative cost under a threshold. However, it is often difficult to effectively capture and enforce hard reachability-based safety constraints indirectly with such constraints on safety violation cost. In this work, we leverage the notion of barrier function to explicitly encode the hard safety chance constraints, and as the environment is unknown, relax them to our design of generativemodel-based soft barrier functions. Based on such soft barriers, we propose a novel safe RL approach with bi-level optimization that can jointly learn the unknown environment and optimize the control policy, while effectively avoiding the unsafe region with safety probability optimization. Experiments on a set of examples demonstrate that our approach can effectively enforce hard safety chance constraints and significantly outperform CMDP-based baseline methods in system safe rates measured via simulations.

## Introduction

Reinforcement learning (RL) has shown promising successes in learning complex policies for games (Silver et al., 2018), robots (Zhao et al., 2020; Yang et al., 2023), and cyber-physical systems like smart buildings (Wei et al., 2017; Xu et al., 2021a; 2022), by maximizing a cumulative reward objective as the optimization goal. However, real-world safety-critical applications, such as autonomous cars (Liu et al., 2022; 2023b;a), still hesitate to adopt RL policies due to safety concerns. In particular, when the environment is stochastic and unknown (Zhu et al., 2020; 2021), these applications often have hard safety chance constraints that require the probability of the system state not reaching certain specified unsafe regions above a threshold, e.g., autonomous cars not deviating into adjacent lanes or UAVs not colliding with trees. It is very challenging to learn a policy via RL that can meet such hard safety chance constraints.

In the literature, the Constrained Markov Decision Process (CMDP) (Altman, 1999) is a popular paradigm for addressing RL safety. Common CMDP-based methods encode safety constraints through a cost function of safety violations, and reduce the policy search space to where the expectation of cumulative discounted cost is less than a threshold. Various RL algorithms are proposed to adaptively solve CMDP through the primal-dual approach for the Lagrangian problem of CMDP. However, it is often hard for CMDP-based methods to enforce reachability-based hard safety chance constraints (i.e., the probability bound of the system state not reaching unsafe regions) with the indirect constraints on the expectation of cumulative cost. In particular, while reachability-based safety constraints are defined on the system state at the time point level (i.e., each point on the trajectory,), the CMDP constraints only enforce the cumulative behavior in expectation at the trajectory level. In other words, the cost penalty on the system visiting the unsafe regions at a certain time point may be offset by the low cost at other times. There is a recent CMDP approach addressing hard safety constraints by using the indicator function for encoding failure probability (Wagener et al., 2021), but it requires a safe backup policy for intervention, which is difficult to achieve in unknown environments. Safe exploration with hard safety constraints has also been studied in (Wachi et al., 2018; Turchetta et al., 2016; Moldovan & Abbeel, 2012). However, these works focus on discrete state and action spaces where the hard safety constraints are defined as a set of unsafe state-action pairs that should not be visited, different from the continuous control setting we are considering.

On the other hand, current control-theoretical approaches for model-based safe RL often try to leverage formal methods to handle hard safety constraints, e.g., by establishing safety guarantees through barrier functions or control barrier functions (Luo & Ma, 2021), or by shielding mechanisms based on reachability analysis (Huang et al., 2019; Fan et al., 2020; Huang et al., 2022) to check whether the system may enter the unsafe regions within a time horizon (Bastani et al., 2021; Huang et al., 2020; Wang et al., 2020; 2021a;b;c; 2022). However, these approaches either require explicit known system models for barrier or shielding construction or an initial safe policy to generate safe trajectory data in a deterministic environment. They cannot be applied to the unknown stochastic environments we are addressing.

To overcome the above challenges, we propose a safe RL framework by encoding the hard safety chance constraints via the learning of a generative-model-based soft barrier function. Specifically, we formulate and solve a novel bilevel optimization problem to learn the policy with joint soft barrier function learning, generative modeling, and policy optimization. The soft barrier function provides guidance for avoiding unsafe regions based on safety probability analysis and optimization. The generative model accesses the trajectory data from the environment-policy closed-loop system with stochastic differential equation (SDE) representation to learn the dynamics and stochasticity of the environment. And we further optimize the policy by maximizing the total discounted reward of the sampled synthetic trajectories from the generative model. This joint training framework is fully differentiable and can be efficiently solved via the gradients. Compared to CMDPbased methods, our approach more directly encodes the hard safety chance constraints along each point of the agent trajectory through the soft barrier function, as shown in Figure 1. While given the unknown stochastic environment, our approach cannot provide a hard barrier and hence no deterministic safety guarantee, experimental results demonstrate that in simulations, ours can significantly outperform the CMDP-based baselines in system safe rate.

The paper is organized as follows. Section 2 introduces related works, Section 3 presents our approach, including the bi-level optimization formulation, our safe RL algorithm with generative modeling, soft barrier function learning, and policy optimization to solve the formulation and theoretical analysis of safety probability. Section 4 shows the experiments and Section 5 concludes the paper.