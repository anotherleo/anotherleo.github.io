# QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks

Deep learning has become a promising programming paradigm in software development, owing to its surprising performance in solving many challenging tasks. Deep neural networks (DNNs) are increasingly being deployed in practice, but are limited on resource-constrained devices owing to their demand for computational power. Quantization has emerged as a promising technique to reduce the size of DNNs with comparable accuracy as their floating-point numbered counterparts. The resulting quantized neural networks (QNNs) can be implemented energy-efficiently. Similar to their floating-point numbered counterparts, quality assurance techniques for QNNs, such as testing and formal verification, are essential but are currently less explored. In this work, we propose a novel and efficient formal verification approach for QNNs. In particular, we are the first to propose an encoding that reduces the verification problem of QNNs into the solving of integer linear constraints, which can be solved using off-the-shelf solvers. Our encoding is both sound and complete. We demonstrate the application of our approach on local robustness verification and maximum robustness radius computation. We implement our approach in a prototype tool QVIP and conduct a thorough evaluation. Experimental results on QNNs with different quantization bits confirm the effectiveness and efficiency of our approach, e.g., two orders of magnitude faster and able to solve more verification tasks in the same time limit than the state-of-the-art methods.



## RELATED WORK

There is a large and growing body of work on quality assurance techniques for (quantized) DNNs including testing (e.g., [7, 11, 48, 49, 56, 61, 66, 71, 74, 79, 82, 83, 85, 88]) and formal verification (e.g., [6, 21, 22, 25, 27, 29, 34, 38, 39, 46, 47, 63, 69, 77, 78, 84, 89]). While testing techniques are often effective in **finding violations of properties**, they cannot prove their absence; formal verification and testing are normally complementary. In this section, we mainly discuss the existing formal verification techniques for (quantized) DNNs, which are classified into the following categories: constraint solving based, abstraction based, and decision diagram based.

### Constraint solving based methods

Early work on formal verification of real numbered DNNs typically reduces the problem to the solving of Satisfiability Modulo Theory (SMT) problem [38, 39, 63] or Mixed Integer Linear Programming (MILP) problem [16, 20, 24], that can be solved by off-the-shelf or dedicated solvers. In theory, such techniques are both sound and complete unless abstraction techniques are adopted to improve scalability. However, verification tools for real numbered DNNs cannot be used to guarantee the robustness of quantized DNNs (i.e., QNNs) due to the fixed-point semantics of QNNs [26].

Along this line, Narodytska et al. [54] proposed to reduce the verification problem of 1-bit quantized DNNs (i.e., BNNs) to the satisfiability problem of Boolean formulas (SAT) or the solving of integer linear constraints. Our QNN encoding can be seen as a non-trivial generalization of their BNN encoding [54]. Furthermore, we also propose to leverage interval analysis to effectively reduce the size of constraints and verification cost. Using a similar encoding of [54], Baluta et al. proposed a PAC-style quantitative analysis framework for BNNs [8] by leveraging approximate SAT model-counting solvers. Jia and Rinard extended the encoding to three-valued BNNs [36]. Narodytska et al. proposed a SAT-based verification-friendly BNN training framework [55].

Recently, accounting for the fixed-point semantics of QNNs, Giacobbe et al. [26] pushed this direction further by introducing the first formal verification approach for multiple-bit quantized DNNs (i.e., QNNs). They encode the verification problem of QNNs in first-order theory of quantifier-free bit-vector with binary representation. Later, first-order theory of fixed-point was proposed and used to verify QNNs [9]. Henzinger et al. [32] explored several heuristics to improve efficiency and scalability of the SMT-based approach [26]. To the best our knowledge, we are the first to reduce the verification problem of QNNs to the solving of integer linear constraints. Experimental results show that our approach is significantly faster than the state-of-the-art [32].

### Abstraction based methods

To improve efficiency and scalability, various abstraction techniques have been used, which typically compute conservative bounds on the value ranges of the neurons for an input region of interest. Abstract interpretation [19] has been widely used by exploring networks layer by layer [4, 25, 43, 44, 6769, 76â€“78, 84], typically with different abstract domains. Almost all the existing work considered real numbered DNNs while few work (e.g., [69]) considered floating-point numbered DNNs. The interval analysis adopted in this work is an application of abstract interpretation with interval as its abstract domain. It is an interesting future work to study other abstract interpretation based techniques to reduce the size of integer linear constraints. Another direction is to abstract neural networks [6, 22, 57], rendering them suitable for formal verification. Abstraction based methods are sound but incomplete, so refinement techniques are normally needed to tighten bounds or refine over-simplified networks.

While widely used for verifying DNNs, abstraction based verification of both QNNs and BNNs is very limited, except for the work [32] which, similar to ours, is used to reduce the size of SMT formulas and verification cost.

Differential verification [41] initially proposed for verifying a new version of a program w.r.t. a previous version, has been applied to QNNs [50, 59, 60]. In general, they check if two (quantified) DNNs with the same architecture but different parameters output similar results (e.g., bounded by a small value) for each input from an input region. As mentioned previously, these techniques cannot be used to directly and precisely verify the robustness of QNNs.

### Decision diagram based methods

Decision diagram based verification methods were proposed for behavior analysis and verification of BNNs [17, 64, 65, 86], e.g., allowing one to reason about the distribution of the adversarial examples or give an interpretation on the decision made by a BNN. Choi et al. [17] proposed a knowledge compilation based encoding that first transforms a BNN into a tractable Boolean circuit and then into a more tractable circuit, called Sentential Decision Diagram (SDD). Based on the SDD representation, polynomial-time queries to SDD can be utilized to explain and verify the behaviors of BNNs efficiently. In parallel, Shih et al. proposed a quantitative verification framework for BNNs [64, 65], where a given BNN with an input region of interest is modeled using a Binary Decision Diagram (BDD) by leveraging BDD-learning [53], which has limited scalability. Very recently, Zhang et al. [86] proposed a novel BDD-based verification framework for BNNs, which exploits the internal structure of the BNNs to construct BDD models instead of BDD-learning. Specifically, they translated the input-output relation of blocks in BNNs to cardinality constraints which can then be encoded by BDDs. Though decision diagram based methods enable behavior analysis and quantitative verification, they can hardly be extended to QNNs, i.e., multiple-bit quantized DNNs, due to the large space of QNNs.

## CONCLUSION

We have proposed the first ILP-based analysis approach for QNNs. We presented a prototype tool QVIP and conducted thorough experiments on various QNNs with different quantization bit sizes. Experimental results showed that QVIP is more efficient and scalable than the state-of-the-art, enabling the computation of maximum robustness radii for QNNs which can be used as a metric for robustness evaluation of QNNs. We also found that the accuracy of QNNs stays similar under different quantization bits, but the robustness can be greatly improved with more quantization bits, using quantization-aware training while it is not using post-training quantization.

We have proposed the first ILP-based analysis approach for QNNs. We presented a prototype tool QVIP and conducted thorough experiments on various QNNs with different quantization bit sizes. Experimental results showed that QVIP is more efficient and scalable than the state-of-the-art, enabling the computation of maximum robustness radii for QNNs which can be used as a metric for robustness evaluation of QNNs. We also found that the accuracy of QNNs stays similar under different quantization bits, but the robustness can be greatly improved with more quantization bits, using quantization-aware training while it is not using post-training quantization.

