# Neuron Semantic-Guided Test Generation for Deep Neural Networks Fuzzing

LI HUANG, WEIFENG SUN, and MENG YAN, Chongqing University, Chongqing, China<br/>
ZHONGXIN LIU, Zhejiang University, Hangzhou, China<br/>
YAN LEI, Chongqing University, Chongqing, China<br/>
DAVID LO, Singapore Management University, Singapore, Singapore

In recent years, significant progress has been made in testing methods for deep neural networks (DNNs) to ensure their correctness and robustness. Coverage-guided criteria, such as neuron-wise, layer-wise, and path-/trace-wise, have been proposed for DNN fuzzing. However, existing coverage-based criteria encounter performance bottlenecks for several reasons: 

- ① Testing Adequacy: Partial neural coverage criteria have been observed to achieve full coverage using only a small number of test inputs. In this case, increasing the number of test inputs does not consistently improve the quality of models. 
- ② Interpretability: The current coverage criteria lack interpretability. Consequently, testers are unable to identify and understand which incorrect attributes or patterns of the model are triggered by the test inputs. This lack of interpretability hampers the subsequent debugging and fixing process. 

Therefore, there is an urgent need for a novel fuzzing criterion that offers improved testing adequacy, better interpretability, and more effective failure detection capabilities for DNNs.

To alleviate these limitations, we propose NSGen, an approach for DNN fuzzing that utilizes neuron semantics as guidance during test generation. NSGen identifies critical neurons, translates their high-level semantic features into natural language descriptions, and then assembles them into human-readable DNN decision paths (representing the internal decision of the DNN). With these decision paths, we can generate more fault-revealing test inputs by quantifying the similarity between original test inputs and mutated test inputs for fuzzing. We evaluate NSGen on popular DNN models (VGG16_BN, ResNet50, and MobileNet_v2) using CIFAR10, CIFAR100, Oxford 102 Flower, and ImageNet datasets. Compared to 12 existing coverageguided fuzzing criteria, NSGen outperforms all baselines, increasing the number of triggered faults by 21.4% to 61.2% compared to the state-of-the-art coverage-guided fuzzing criterion. This demonstrates NSGen’s effectiveness in generating fault-revealing test inputs through guided input mutation, highlighting its potential to enhance DNN testing and interpretability.

## Introduction

Deep Neural Networks (DNNs) have witnessed remarkable advancements over the past few decades and are now extensively employed in diverse applications such as image classification [19], computer vision [31], speech recognition [2], natural language processing [20], and medical diagnosis [18, 40]. Despite their impressive performance, concerns about the safety and robustness of DNNs have been raised, especially in safety-critical applications like autonomous driving [3]. Any unexpected misbehavior of DNNs may lead to catastrophic consequences, making it essential to test DNNs and effectively identify their defects.

Fuzzing, as a well-established automatic testing technique [9], has been demonstrated to be effective in detecting bugs and vulnerabilities in traditional software systems. Fuzzing involves generating random test inputs for the software under test, monitoring the application’s behavior during test execution, collecting test inputs’ execution information, and mutating the inputs to trigger faults based on specific coverage criteria. While traditional Coverage-Guided Fuzzing (CGF) methods [12, 17, 65] are effective, applying such techniques directly to DNNs poses challenges due to inherent differences in test input mutation strategies and feedback guidance between DNNs and traditional software. Therefore, the design of an effective fuzzing strategy becomes crucial in the context of DNN testing, with feedback guidance, i.e., coverage criteria, playing a pivotal role in effectively uncovering faults in DNNs.

To this end, several neural coverage criteria have been proposed for DNNs based on the neural activation status [59]. Such criteria can be broadly categorized into neuron-wise, layer-wise, and trace-/path-wise criteria. Neuron-wise criteria [45, 59] evaluate test input coverage by considering each neuron individually, while layer-wise criteria [45] assess coverage from a layer-level perspective. Trace-/path-wise criteria [37, 39, 81] measure DNN coverage based on the traces or paths traversed by neurons. These criteria encompass various coverage calculations, such as Neuron Coverage (NC) [59], Top-k NC (TKNC) [45], and neuron path coverage [81]. Guided by existing coverage criteria, automated testing techniques like DeepXplore [59], DeepTest [77], and DeepHunter [82] have been developed to generate test inputs that maximize the above-mentioned NLC. However, recent studies [30, 47, 69, 87, 88] have highlighted the limitations of existing NLC criteria in guiding the generation of DNN test inputs: ¿ Testing adequacy. It has been observed that a partial NLC criterion could achieve full coverage using only a small number of test inputs. In this case, increasing the number of test inputs did not consistently improve the quality of models [30, 47]. ¡ Interpretability. The existing coverage criteria lack interpretability [47, 69]. As a result, testers are unable to identify what incorrect attributes or patterns of the model are triggered by the test inputs. Additionally, this lack of interpretability hinders the subsequent debugging and fixing process, as it becomes challenging to understand and address the underlying bugs in the model.

## References

