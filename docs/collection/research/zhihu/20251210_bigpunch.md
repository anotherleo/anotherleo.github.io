把验证集混进测试集，或者搞人工干预数据强行刷点，这是在给自己埋雷。这颗雷在你发论文的时候可能运气好一时没有暴雷，可能在你毕业答辩的时候可能也没有爆炸，但是它一定会始终埋伏在此后职业生涯里，说不定就会在某一个关键节点把你炸得粉身碎骨。

你现在觉得只要结果好看了，idea就能被认可，论文就能发，毕业就能稳。这是一种极其短视的线性思维。在工业界和真正的顶级学术圈，大家看的从来不仅仅是那个加粗的SOTA指标，大家看的是你对数据的理解，对Corner Case的分析，以及你那个idea到底是在解决什么问题。

很多时候我们觉得自己的idea特别好，那是一种自我感动。比如你设计了一个极其复杂的Attention机制，你觉得这符合人类认知过程，逻辑完美。但是模型不买账。

深度学习这东西，本质上是基于统计的经验主义。它不讲逻辑，它讲概率分布。你觉得好的idea，可能引入了额外的计算负担，可能导致了梯度消失，可能破坏了特征空间的连续性。

如果实验结果不好，最大的可能性有三个：

第一，你的代码写错了。这是最常见的情况。我在团队里Review代码，百分之八十的情况是数据预处理管道里出了问题，比如掩码没盖住，比如归一化做反了，比如DataLoader里把标签搞乱了。这时候你应该做的是去Debug，而不是去改数据。推荐你没事多去看看 **Karpathy写的A Recipe for Training Neural Networks** ，这简直就是炼丹师的圣经。他在里面非常详细地讲了怎么从过拟合一个小数据集开始，一步步排查代码问题。很多时候，你以为是模型不行，其实是实现细节里的bug。这就像你觉得自己设计了一辆法拉利，结果跑不过五菱宏光，你第一反应不应该是要把终点线往前挪，而是应该检查一下你是不是忘了松手刹。

第二，你的Baseline太强了。现在很多领域的SOTA都已经刷到了天花板，比如ResNet或者Transformer的变种，那是全世界顶尖脑袋优化了多少年的结果。你的idea如果是加个小模块，或者改个损失函数，提升不上去是很正常的。这时候你要做的不是造假，而是去分析你的模型在哪些样本上失败了。

这里我必须要提到 **吴恩达的Machine Learning Yearning** 这本书，虽然有点年头了，但是里面的误差分析Error Analysis章节是永不过时的。很多人做算法只看个Accuracy或者F1 Score，从来不去看Bad Case。你把那些预测错的图片或者文本拿出来，肉眼看一百条，你立刻就能知道你的模型到底傻在哪里。是光照问题？是遮挡问题？还是标注本身就有歧义？做这种分析写进论文里，比单纯刷高0.5个点要有价值得多。

第三，也是最残酷的，你的idea确实不行。深度学习里有个很有意思的现象，很多听起来很反直觉的方法反而有效，很多听起来很符合逻辑的方法反而无效。这时候你要做的是Ablation Study，消融实验。控制变量，把你idea的各个部分拆开来测。如果你发现你的核心模块加上去之后效果反而降了，那就诚实地记录下来。现在的学术界也开始反思唯SOTA论，一篇详实分析为什么某个直觉上合理的idea无效的论文，如果分析得足够深入，同样是有价值的。

你说你想偷偷把验证集放点到测试集里。这叫Data Leakage，数据泄露。

在工业界，这属于重大生产事故。

我给你讲个真实的案例。前几年面试过一个名校的硕士，简历非常漂亮，论文发了顶会，在很多榜单上都刷到了第一。我让他讲讲他的模型，讲得头头是道。然后我问了一个很细节的问题：你的模型在训练集和测试集上的分布差异大吗？你是怎么处理OOD（Out of Distribution）情况的？

他支支吾吾答不上来。我让他现场手写一段数据划分的代码，他写的代码里随机种子没固定，而且在归一化的时候，竟然是用整个数据集的均值方差来做Normalize，而不是只用训练集的。

这就意味着，测试集的信息被泄露到了训练过程中。这种模型在离线测试的时候指标逆天，一旦上线，面对全新的、从未见过的数据，效果会直接崩盘。

我们当时没有录用他。因为对于公司来说，招一个技术平庸的人也就是浪费点工资，但招一个没有数据洁癖、甚至试图糊弄数据的人，可能会导致整个推荐系统、风控系统的逻辑崩盘，给公司造成几百万甚至上亿的损失。

一旦你开了这个口子，你的阈值就会越来越低。今天你往测试集里掺了10%的训练数据，明天你就敢直接改Excel里的结果。等到你习惯了这种获得正反馈的捷径，你就再也沉不下心去啃硬骨头了。

算法工程师的核心竞争力，不是你会调包，不是你会改模型结构，而是你对数据的敬畏和敏感度。

你说你想人工干预。这在某些特定场景下叫Hard Sample Mining，难例挖掘，那是合法的优化手段，比如把模型做错的样本加权重新训练。但如果你是指直接手动修改预测结果来计算准确率，那就是欺诈。

在这个圈子里，名声比什么都重要。咱们这个行业说大很大，说小很小。你发了论文，代码不开源，或者开源的代码复现不了结果，大家心里都跟明镜似的。以后你找工作，人家背景调查一做，发现你的成果水分很大，你觉得还有人敢要你吗？

我知道你很急，论文Deadline就在眼前，或者项目马上要汇报。这时候效果不好，怎么办？

我给你几条实操性极强的建议，这都是我在一线带项目磨出来的经验。

你要重新审视你的数据集。很多时候模型学不好，是因为数据本身就是脏的。Label Noise标签噪声是一个非常普遍的问题。你去看看 **Cleanlab** 这个开源库，它专门用来帮你在数据集中找错误的标注。我曾经在一个项目里，死活跑不出效果，后来用类似的思想把数据集清洗了一遍，发现有15%的标注是错的。把这些错标数据剔除或者修正后，模型效果直接起飞。这不叫造假，这叫数据清洗，是数据科学家最高级的工作之一。

你要关注数据的分布。是不是训练集和测试集的分布不一致？比如训练集全是白天的图片，测试集全是晚上的。这种情况下，你要做的是Domain Adaptation领域自适应，或者把训练集做增强。数据增强Data Augmentation是合法的提分手段。你去做旋转、剪切、颜色变换，或者用Mixup之类的方法。推荐你去读一下 **Bag of Tricks for Image Classification** 这篇论文，虽然是讲图像分类的，但是里面的思想，比如Label Smoothing、Mixup、Cosine Learning Rate Decay，在很多任务里都是通用的提分利器。

你要学会做负结果分析。如果你的idea真的不work，但是你又必须发论文或者结项，你可以把重点从 我提出了一个SOTA模型 转移到 我深入探究了为什么这类方法在当前数据下失效。你可以画出特征空间的分布图（用t-SNE可视化），证明你的改动虽然没有提升整体准确率，但是改善了类内距离，或者在某些长尾类别上有显著提升。

现在的评审口味也在变。大家看腻了那些莫名其妙堆砌模块刷点的论文。如果能看到一篇深入剖析数据特性、模型行为的文章，反而是清流。

再给你推荐一个资源， **Papers with Code** 这个网站。不要只盯着上面的榜首看，你要去看那些榜单下面链接的GitHub仓库，看看别人的Issue区。那里往往藏着最真实的工程细节。你会发现很多SOTA也是有各种Trick堆出来的，但人家是明着堆，不是暗着改数据。你可以学习那些Trick，比如学习率预热Warmup，比如不同层设置不同学习率，这些都是合法的技术手段。

我见过太多聪明反被聪明误的人。当年我在那个搞CV的组，有个哥们为了赶CVPR，把测试集的Label偷偷看了一眼，针对性地调了一组超参数。论文是中了。但是后来那篇论文的代码开源后，被人提Issue说复现不出来。他只好装死。哪怕后来他跳槽去了大厂，这件事也像个幽灵一样跟着他。每次只要有人提起那篇论文，大家都会意味深长地笑一笑。他在业内的口碑就这么烂了。

相反，我带过一个实习生，做序列推荐的。他的idea一开始跑输了Baseline。他没有灰心，也没有动歪脑筋。他花了两周时间，写了大量的脚本去统计训练集和测试集里Item的流行度分布，最后发现是他的模型对冷门物品过拟合了。他把这个发现写进了报告，虽然最后的指标只是持平了Baseline，但他对数据的深入洞察打动了答辩评委。现在这小伙子已经是某独角兽公司的算法Tech Lead了。

因为他展现出了解决问题的能力，而不是掩盖问题的能力。

最近几年，吴恩达一直在推Data-Centric AI的概念。我觉得这才是正道。模型结构大家都卷得差不多了，未来的竞争在于谁能更好地理解和利用数据。

如果你的模型效果不好，不要总是想着改模型，去看看数据。

你可以尝试用Active Learning主动学习的思路，去识别出那些模型最不确定的样本，然后人工去check这些样本的标注是不是有问题，或者这些样本是不是太难了。

你可以尝试用Self-Supervised Learning自监督学习的方法，在有限的标签数据之外，利用大量的无标签数据来预训练你的模型。这比你偷偷改测试集要高级得多了，而且是目前最火的研究方向。你去看看 **SimCLR** 或者 **MAE** 这些工作，全是靠挖掘数据自身的信息来提升效果。

甚至，你可以尝试做Ensemble模型融合。搞它五个十个不同的模型，最后取平均或者投票。在比赛里这叫刷榜大杀器，在工业界这也是保证稳定性的常规操作。这也是合法的。

很多时候我们焦虑，是因为我们把手段当成了目的。跑模型、发论文，这些都是手段，目的是什么？是探索未知的规律，是解决实际的问题。

当你动了造假的念头，你就已经放弃了探索，放弃了解决问题，你只想解决那个指标。

你想想，如果你去医院看病，医生为了指标好看，把你的化验单改了，告诉你没事，结果你病入膏肓。你现在的行为，就是那个改化验单的医生。

IT互联网这个行业，迭代极快。今天的SOTA，明天就是Baseline，后天就是Legacy。你造假换来的那点虚荣，保质期短得可怜。但是你通过扎扎实实的Debug、分析数据、阅读源码沉淀下来的Debug直觉和工程能力，是会伴随你整个职业生涯的护城河。

所以，我的建议非常直接：

把那点小心思收起来。

关掉那个想改数据的Excel。

打开你的TensorBoard，一行一行看日志。  
打开你的Bad Case列表，一张一张看图片。  
打开你的代码，一行一行Review逻辑。

如果不work，就承认它不work，然后分析为什么。这才是数据科学家该有的样子。

如果实在没招了，哪怕换个简单的Baseline，把故事讲圆，把实验做得扎实，也比搞个假的SOTA强一万倍。

最后送你一句话：技术是诚实的，由于随机性的存在，它偶尔会欺骗你，但如果你试图欺骗它，它早晚会报复你。

---

原文地址：[(bigpunch)深度学习里明明有一个很好的idea，但是跑出的效果不理想，是否可以稍微人工干预?](https://www.zhihu.com/question/666647497/answer/1982175646961665485) 

